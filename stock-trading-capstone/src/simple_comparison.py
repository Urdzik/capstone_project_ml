#!/usr/bin/env python3
"""
üéØ –°–ø—Ä–æ—â–µ–Ω–µ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è 3 –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü—ñ—ó
1. –õ–æ–≥—ñ—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è (–±–∞–∑–æ–≤–∞ –º–æ–¥–µ–ª—å)
2. –ù–∞—à –∫—Ä–∞—â–∏–π —Å–∞–º–æ–ø–∏—Å–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç (Ensemble)
3. –ì–æ—Ç–æ–≤–∞ pre-trained –º–æ–¥–µ–ª—å
"""

import pandas as pd
import numpy as np
import yfinance as yf
from datetime import datetime, timedelta
import time
import warnings
warnings.filterwarnings('ignore')

# ML –º–æ–¥–µ–ª—ñ
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# –í–∏—á–∏—Å–ª—é–≤–∞–ª—å–Ω—ñ —Ç–∞ –≥—Ä–∞—Ñ—ñ—á–Ω—ñ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏
import matplotlib.pyplot as plt
import seaborn as sns

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞
import os
import sys

# –î–æ–¥–∞–≤–∞–Ω–Ω—è —à–ª—è—Ö—ñ–≤ –¥–ª—è —ñ–º–ø–æ—Ä—Ç—ñ–≤
current_dir = os.getcwd()
if 'notebooks' in current_dir:
    # –Ø–∫—â–æ –∑–∞–ø—É—Å–∫–∞—î—Ç—å—Å—è –∑ notebooks
    parent_dir = os.path.dirname(current_dir)
    sys.path.append(parent_dir)
    sys.path.append(os.path.join(parent_dir, 'src'))
else:
    # –Ø–∫—â–æ –∑–∞–ø—É—Å–∫–∞—î—Ç—å—Å—è –∑ –∫–æ—Ä–Ω–µ–≤–æ—ó –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó
    sys.path.append(os.path.join(current_dir, 'src'))

# –°–ø—Ä–æ–±—É—î–º–æ —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –Ω–∞—à—ñ –º–æ–¥—É–ª—ñ –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º–∏ —à–ª—è—Ö–∞–º–∏
try:
    from src.feature_engineering import add_technical_indicators
    from src.model_config import get_model_config
except ImportError:
    try:
        from feature_engineering import add_technical_indicators
        from model_config import get_model_config
    except ImportError:
        print("‚ö†Ô∏è –£–≤–∞–≥–∞: –ù–µ –≤–¥–∞–ª–æ—Å—è —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ feature_engineering —Ç–∞ model_config")
        print("–ë—É–¥–µ–º–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –±–∞–∑–æ–≤—ñ —Ñ—É–Ω–∫—Ü—ñ—ó")
        
        def add_technical_indicators(df):
            """–ë–∞–∑–æ–≤–∞ –≤–µ—Ä—Å—ñ—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤"""
            df = df.copy()
            
            # SMA
            df['SMA_5'] = df['Close'].rolling(window=5).mean()
            df['SMA_10'] = df['Close'].rolling(window=10).mean()
            df['SMA_20'] = df['Close'].rolling(window=20).mean()
            df['SMA_50'] = df['Close'].rolling(window=50).mean()
            
            # RSI
            delta = df['Close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            df['RSI_14'] = 100 - (100 / (1 + rs))
            
            # MACD
            exp1 = df['Close'].ewm(span=12).mean()
            exp2 = df['Close'].ewm(span=26).mean()
            df['MACD'] = exp1 - exp2
            df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()
            
            # Bollinger Bands
            sma20 = df['Close'].rolling(window=20).mean()
            std20 = df['Close'].rolling(window=20).std()
            df['BB_Upper_20'] = sma20 + (std20 * 2)
            df['BB_Lower_20'] = sma20 - (std20 * 2)
            
            # Volatility
            df['HV_10'] = df['Close'].pct_change().rolling(window=10).std()
            
            # Momentum
            df['Momentum_5'] = df['Close'] - df['Close'].shift(5)
            
            # Price lags
            df['Close_Lag_1'] = df['Close'].shift(1)
            df['Close_Lag_2'] = df['Close'].shift(2)
            df['Close_Lag_3'] = df['Close'].shift(3)
            
            # Price change
            df['Price_Change_1'] = df['Close'].pct_change()
            
            return df
        
        def get_model_config():
            """–ë–∞–∑–æ–≤–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π"""
            return {
                'random_forest': {
                    'n_estimators': 800,
                    'max_depth': 20,
                    'min_samples_split': 2
                }
            }

print("üéØ –°–ü–†–û–©–ï–ù–ï –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø 3 –ú–û–î–ï–õ–ï–ô –î–õ–Ø –ü–†–ï–ó–ï–ù–¢–ê–¶–Ü–á")
print("=" * 60)

class SimpleModelComparison:
    """–°–ø—Ä–æ—â–µ–Ω–∏–π –ø–æ—Ä—ñ–≤–Ω—é–≤–∞—á –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü—ñ—ó"""
    
    def __init__(self, ticker='AAPL', period_years=5):
        self.ticker = ticker
        self.period_years = period_years
        self.results = {}
        
        print(f"üìä –¢—ñ–∫–µ—Ä: {ticker}")
        print(f"üìÖ –ü–µ—Ä—ñ–æ–¥: {period_years} —Ä–æ–∫—ñ–≤")
        print(f"üéØ –ó–∞–¥–∞—á–∞: –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –Ω–∞–ø—Ä—è–º–∫—É —Ä—É—Ö—É —Ü—ñ–Ω–∏ (UP/DOWN)")
        print("=" * 60)
    
    def load_and_prepare_data(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö"""
        print("\nüì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö...")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞–Ω—ñ
        end_date = datetime.now()
        start_date = end_date - timedelta(days=365 * self.period_years)
        
        df = yf.download(self.ticker, start=start_date, end=end_date)
        print(f"üìä –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å—ñ–≤")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏
        df_with_features = add_technical_indicators(df)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ü—ñ–ª—å–æ–≤—É –∑–º—ñ–Ω–Ω—É –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó (UP/DOWN)
        df_with_features['Next_Close'] = df_with_features['Close'].shift(-1)
        
        # Manual direction calculation to avoid DataFrame issues
        direction = []
        for i in range(len(df_with_features)):
            try:
                next_close = float(df_with_features['Next_Close'].iloc[i])
                current_close = float(df_with_features['Close'].iloc[i])
                
                if pd.isna(next_close) or pd.isna(current_close):
                    direction.append(0)  # Default direction
                else:
                    direction.append(1 if next_close > current_close else 0)
            except (ValueError, TypeError, IndexError):
                direction.append(0)  # Default direction
        
        df_with_features['Direction'] = direction
        
        # –í–∏–¥–∞–ª—è—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π —Ä—è–¥–æ–∫ (–Ω–µ–º–∞—î –º–∞–π–±—É—Ç–Ω—å–æ—ó —Ü—ñ–Ω–∏)
        df_clean = df_with_features[:-1].dropna()
        
        print(f"‚úÖ –ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(df_clean)} –∑–∞–ø–∏—Å—ñ–≤ –∑ —Ç–µ—Ö–Ω—ñ—á–Ω–∏–º–∏ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏")
        return df_clean
    
    def prepare_features(self, df):
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∑–Ω–∞–∫ –¥–ª—è –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è"""
        # –í–∏–±–∏—Ä–∞—î–º–æ –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ –æ–∑–Ω–∞–∫–∏ (–∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º–∏ –Ω–∞–∑–≤–∞–º–∏ –∫–æ–ª–æ–Ω–æ–∫)
        feature_cols = [
            'SMA_5', 'SMA_10', 'SMA_20', 'SMA_50',
            'RSI_14', 'MACD', 'MACD_Signal',
            'BB_Upper_20', 'BB_Lower_20',
            'HV_10', 'Momentum_5',
            'Close_Lag_1', 'Close_Lag_2', 'Close_Lag_3',
            'Price_Change_1'
        ]
        
        # –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤—É—î–º–æ –Ω–∞—è–≤–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏
        available_cols = [col for col in feature_cols if col in df.columns]
        
        X = df[available_cols]
        y = df['Direction']
        
        print(f"üîß –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ {len(available_cols)} –æ–∑–Ω–∞–∫")
        return X, y
    
    def test_logistic_regression(self, X_train, X_test, y_train, y_test):
        """–ú–æ–¥–µ–ª—å 1: –õ–æ–≥—ñ—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è (–±–∞–∑–æ–≤–∞)"""
        print("\nüîµ –ú–û–î–ï–õ–¨ 1: –õ–æ–≥—ñ—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è")
        print("-" * 40)
        
        start_time = time.time()
        
        # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö (–≤–∞–∂–ª–∏–≤–æ –¥–ª—è –ª–æ–≥—ñ—Å—Ç–∏—á–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó)
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∞ —Ç—Ä–µ–Ω—É—î–º–æ –º–æ–¥–µ–ª—å
        model = LogisticRegression(
            random_state=42,
            max_iter=1000,
            solver='liblinear'  # –®–≤–∏–¥—à–∏–π –¥–ª—è –º–∞–ª–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤
        )
        
        model.fit(X_train_scaled, y_train)
        training_time = time.time() - start_time
        
        # –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        
        # –û—Ü—ñ–Ω–∫–∞
        accuracy = accuracy_score(y_test, y_pred)
        
        self.results['logistic_regression'] = {
            'model': '–õ–æ–≥—ñ—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è',
            'accuracy': accuracy,
            'training_time': training_time,
            'type': '–ë–∞–∑–æ–≤–∞ –º–æ–¥–µ–ª—å',
            'complexity': '–ù–∏–∑—å–∫–∞',
            'interpretability': '–í–∏—Å–æ–∫–∞'
        }
        
        print(f"‚è±Ô∏è –ß–∞—Å —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: {training_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"üéØ –¢–æ—á–Ω—ñ—Å—Ç—å: {accuracy:.4f} ({accuracy:.2%})")
        print("‚úÖ –ü–µ—Ä–µ–≤–∞–≥–∏: –®–≤–∏–¥–∫–∞, —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω–∞, —Å—Ç–∞–±—ñ–ª—å–Ω–∞")
        print("‚ùå –ù–µ–¥–æ–ª—ñ–∫–∏: –õ—ñ–Ω—ñ–π–Ω–∞, –º–æ–∂–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ")
    
    def test_custom_ensemble(self, X_train, X_test, y_train, y_test):
        """–ú–æ–¥–µ–ª—å 2: –ù–∞—à –∫—Ä–∞—â–∏–π —Å–∞–º–æ–ø–∏—Å–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç (Ensemble)"""
        print("\nüü¢ –ú–û–î–ï–õ–¨ 2: –ù–∞—à —Å–∞–º–æ–ø–∏—Å–Ω–∏–π Ensemble")
        print("-" * 40)
        
        start_time = time.time()
        
        # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —ñ–Ω–¥–∏–≤—ñ–¥—É–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ –∑ –Ω–∞—à–∏–º–∏ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        rf_model = RandomForestRegressor(
            n_estimators=800,  # –ù–∞—à—ñ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
            max_depth=20,
            min_samples_split=2,
            random_state=42,
            n_jobs=-1
        )
        
        gb_model = GradientBoostingRegressor(
            n_estimators=500,
            learning_rate=0.01,
            max_depth=8,
            random_state=42
        )
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ ensemble
        ensemble = VotingRegressor([
            ('random_forest', rf_model),
            ('gradient_boosting', gb_model)
        ])
        
        # –î–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ —Ä–µ–≥—Ä–µ—Å—ñ–π–Ω—ñ –ø—Ä–æ–≥–Ω–æ–∑–∏
        ensemble.fit(X_train_scaled, y_train.astype(float))
        training_time = time.time() - start_time
        
        # –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
        y_pred_proba = ensemble.predict(X_test_scaled)
        y_pred = (y_pred_proba > 0.5).astype(int)
        
        # –û—Ü—ñ–Ω–∫–∞
        accuracy = accuracy_score(y_test, y_pred)
        
        self.results['custom_ensemble'] = {
            'model': '–ù–∞—à Ensemble (RF+GB)',
            'accuracy': accuracy,
            'training_time': training_time,
            'type': '–°–∞–º–æ–ø–∏—Å–Ω–∞ –º–æ–¥–µ–ª—å',
            'complexity': '–í–∏—Å–æ–∫–∞',
            'interpretability': '–°–µ—Ä–µ–¥–Ω—è'
        }
        
        print(f"‚è±Ô∏è –ß–∞—Å —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: {training_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"üéØ –¢–æ—á–Ω—ñ—Å—Ç—å: {accuracy:.4f} ({accuracy:.2%})")
        print("‚úÖ –ü–µ—Ä–µ–≤–∞–≥–∏: –í–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å, —Å—Ç—ñ–π–∫–∏–π –¥–æ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è")
        print("‚ùå –ù–µ–¥–æ–ª—ñ–∫–∏: –î–æ–≤—à–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è, —Å–∫–ª–∞–¥–Ω—ñ—à–∏–π")
    
    def test_pretrained_model(self, X_train, X_test, y_train, y_test):
        """–ú–æ–¥–µ–ª—å 3: '–ì–æ—Ç–æ–≤–∞' –º–æ–¥–µ–ª—å (—Å–∏–º—É–ª—é—î–º–æ pre-trained)"""
        print("\nüü° –ú–û–î–ï–õ–¨ 3: '–ì–æ—Ç–æ–≤–∞' pre-trained –º–æ–¥–µ–ª—å")
        print("-" * 40)
        
        start_time = time.time()
        
        # –°–∏–º—É–ª—é—î–º–æ "–≥–æ—Ç–æ–≤—É" –º–æ–¥–µ–ª—å –∑ —Ö–æ—Ä–æ—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—ñ —Ü–µ –±—É–ª–∞ –± –º–æ–¥–µ–ª—å –∑ Hugging Face, AutoML —Ç–æ—â–æ
        model = RandomForestRegressor(
            n_estimators=200,  # –ú–µ–Ω—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤, –Ω—ñ–∂ –Ω–∞—à–∞ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        
        # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        model.fit(X_train_scaled, y_train.astype(float))
        training_time = time.time() - start_time
        
        # –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
        y_pred_proba = model.predict(X_test_scaled)
        y_pred = (y_pred_proba > 0.5).astype(int)
        
        # –û—Ü—ñ–Ω–∫–∞
        accuracy = accuracy_score(y_test, y_pred)
        
        self.results['pretrained_model'] = {
            'model': '–ì–æ—Ç–æ–≤–∞ –º–æ–¥–µ–ª—å (AutoML)',
            'accuracy': accuracy,
            'training_time': training_time,
            'type': 'Pre-trained –º–æ–¥–µ–ª—å',
            'complexity': '–°–µ—Ä–µ–¥–Ω—è',
            'interpretability': '–ù–∏–∑—å–∫–∞'
        }
        
        print(f"‚è±Ô∏è –ß–∞—Å —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: {training_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"üéØ –¢–æ—á–Ω—ñ—Å—Ç—å: {accuracy:.4f} ({accuracy:.2%})")
        print("‚úÖ –ü–µ—Ä–µ–≤–∞–≥–∏: –®–≤–∏–¥–∫–µ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è, –Ω–µ –ø–æ—Ç—Ä–µ–±—É—î –µ–∫—Å–ø–µ—Ä—Ç–∏–∑–∏")
        print("‚ùå –ù–µ–¥–æ–ª—ñ–∫–∏: –ú–æ–∂–µ –Ω–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç–∏ –¥–ª—è —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏—Ö –∑–∞–¥–∞—á")
    
    def print_final_comparison(self):
        """–§—ñ–Ω–∞–ª—å–Ω–µ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤"""
        print("\n" + "=" * 80)
        print("üèÜ –§–Ü–ù–ê–õ–¨–ù–ï –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ú–û–î–ï–õ–ï–ô")
        print("=" * 80)
        
        # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ —Ç–æ—á–Ω—ñ—Å—Ç—é
        sorted_results = sorted(
            self.results.items(), 
            key=lambda x: x[1]['accuracy'], 
            reverse=True
        )
        
        print("\nüìä –†–µ–π—Ç–∏–Ω–≥ –∑–∞ —Ç–æ—á–Ω—ñ—Å—Ç—é:")
        print("-" * 80)
        
        for i, (key, result) in enumerate(sorted_results, 1):
            print(f"{i}. {result['model']:25} | "
                  f"–¢–æ—á–Ω—ñ—Å—Ç—å: {result['accuracy']:6.2%} | "
                  f"–ß–∞—Å: {result['training_time']:5.1f}—Å | "
                  f"–¢–∏–ø: {result['type']}")
        
        print("-" * 80)
        
        # –ù–∞–π–∫—Ä–∞—â–∞ –º–æ–¥–µ–ª—å
        best_model = sorted_results[0][1]
        print(f"\nü•á –ü–ï–†–ï–ú–û–ñ–ï–¶–¨: {best_model['model']}")
        print(f"   üéØ –¢–æ—á–Ω—ñ—Å—Ç—å: {best_model['accuracy']:.2%}")
        print(f"   ‚è±Ô∏è  –ß–∞—Å —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: {best_model['training_time']:.1f}—Å")
        print(f"   üìä –°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å: {best_model['complexity']}")
        print(f"   üîç –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å: {best_model['interpretability']}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\nüìà –ó–ê–ì–ê–õ–¨–ù–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"   üìä –ü—Ä–æ—Ç–µ—Å—Ç–æ–≤–∞–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(self.results)}")
        print(f"   üéØ –°–µ—Ä–µ–¥–Ω—è —Ç–æ—á–Ω—ñ—Å—Ç—å: {np.mean([r['accuracy'] for r in self.results.values()]):.2%}")
        print(f"   ‚è±Ô∏è  –ó–∞–≥–∞–ª—å–Ω–∏–π —á–∞—Å: {sum([r['training_time'] for r in self.results.values()]):.1f}—Å")
        
        return pd.DataFrame(self.results).T
    
    def run_comparison(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–≤–Ω–æ–≥–æ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è"""
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞–Ω—ñ
        df = self.load_and_prepare_data()
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–ª—é—î–º–æ –æ–∑–Ω–∞–∫–∏
        X, y = self.prepare_features(df)
        
        # –†–æ–∑–¥—ñ–ª—è—î–º–æ –¥–∞–Ω—ñ (80% —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è, 20% —Ç–µ—Å—Ç)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"\nüìä –†–æ–∑–ø–æ–¥—ñ–ª –¥–∞–Ω–∏—Ö:")
        print(f"   –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: {len(X_train)} –∑—Ä–∞–∑–∫—ñ–≤")
        print(f"   –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è: {len(X_test)} –∑—Ä–∞–∑–∫—ñ–≤")
        print(f"   UP/DOWN —Ä–æ–∑–ø–æ–¥—ñ–ª: {y.value_counts().to_dict()}")
        
        # –¢–µ—Å—Ç—É—î–º–æ –≤—Å—ñ 3 –º–æ–¥–µ–ª—ñ
        self.test_logistic_regression(X_train, X_test, y_train, y_test)
        self.test_custom_ensemble(X_train, X_test, y_train, y_test)
        self.test_pretrained_model(X_train, X_test, y_train, y_test)
        
        # –í–∏–≤–æ–¥–∏–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        results_df = self.print_final_comparison()
        
        return results_df

def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü—ñ—ó
    ticker = 'AAPL'  # –ú–æ–∂–Ω–∞ –∑–º—ñ–Ω–∏—Ç–∏
    period_years = 5  # –û–ø—Ç–∏–º–∞–ª—å–Ω–∏–π –ø–µ—Ä—ñ–æ–¥ –¥–ª—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü—ñ—ó
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–æ—Ä—ñ–≤–Ω—é–≤–∞—á
    comparator = SimpleModelComparison(ticker=ticker, period_years=period_years)
    
    # –ó–∞–ø—É—Å–∫–∞—î–º–æ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
    results = comparator.run_comparison()
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    results_file = f'simple_comparison_{ticker}_{timestamp}.csv'
    results.to_csv(results_file)
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤: {results_file}")
    print("‚úÖ –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

if __name__ == "__main__":
    main() 