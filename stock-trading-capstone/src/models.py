import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import warnings
from typing import Tuple, Dict, List, Optional, Any
import joblib
from pathlib import Path

warnings.filterwarnings('ignore')

class BaseModel:
    """
    –ë–∞–∑–æ–≤–∏–π –∫–ª–∞—Å –¥–ª—è –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π
    """
    def __init__(self, model_name: str = "BaseModel"):
        self.model_name = model_name
        self.model = None
        self.scaler = None
        self.is_fitted = False
        
    def prepare_data(self, df: pd.DataFrame, target_col: str = 'Close', 
                    exclude_cols: List[str] = None) -> Tuple[pd.DataFrame, pd.Series]:
        """
        –ì–æ—Ç—É—î –¥–∞–Ω—ñ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è (–≤–∏–∫–ª—é—á–∞—î –≤–∏—Ç—ñ–∫ –¥–∞–Ω–∏—Ö)
        """
        if exclude_cols is None:
            exclude_cols = ['Open', 'High', 'Low', 'Volume', 'Adj Close']
        
        # –î–æ–¥–∞—î–º–æ –≤–∏–∫–ª—é—á–µ–Ω–Ω—è –¥–ª—è –≤–∏—Ç–æ–∫—É –¥–∞–Ω–∏—Ö
        data_leakage_patterns = [
            '_Lag_',        # –í–°–Ü lag features (Close_Lag, High_Lag, Low_Lag, etc.)
            'Close_Max_',   # –ú–∞–∫—Å–∏–º—É–º–∏ Close –¥—É–∂–µ –±–ª–∏–∑—å–∫—ñ –¥–æ Close
            'Close_Min_',   # –ú—ñ–Ω—ñ–º—É–º–∏ Close –¥—É–∂–µ –±–ª–∏–∑—å–∫—ñ –¥–æ Close  
            'Close_Mean_',  # –°–µ—Ä–µ–¥–Ω—ñ Close –¥—É–∂–µ –±–ª–∏–∑—å–∫—ñ –¥–æ Close
            'Close_Median_', # –ú–µ–¥—ñ–∞–Ω–∏ Close –¥—É–∂–µ –±–ª–∏–∑—å–∫—ñ –¥–æ Close
            'EMA_',         # –ï–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω—ñ –∫–æ–≤–∑–Ω—ñ —Å–µ—Ä–µ–¥–Ω—ñ –¥—É–∂–µ –±–ª–∏–∑—å–∫—ñ –¥–æ Close
            'SMA_',         # –ü—Ä–æ—Å—Ç—ñ –∫–æ–≤–∑–Ω—ñ —Å–µ—Ä–µ–¥–Ω—ñ –¥—É–∂–µ –±–ª–∏–∑—å–∫—ñ –¥–æ Close
            'PSAR',         # Parabolic SAR –¥—É–∂–µ –±–ª–∏–∑—å–∫–∏–π –¥–æ Close
            'BB_Middle_',   # –°–µ—Ä–µ–¥–∏–Ω–∞ Bollinger Bands = SMA
            'VPT',          # Volume Price Trend –¥—É–∂–µ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏–π
        ]
        
        feature_cols = []
        for col in df.columns:
            if col in exclude_cols + [target_col]:
                continue
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –Ω–µ –º—ñ—Å—Ç–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω–∏ –≤–∏—Ç–æ–∫—É –¥–∞–Ω–∏—Ö
            if any(pattern in col for pattern in data_leakage_patterns):
                continue
            feature_cols.append(col)
        
        X = df[feature_cols].dropna()
        y = df.loc[X.index, target_col]
        
        print(f"–í–∏–∫–ª—é—á–µ–Ω–æ –æ–∑–Ω–∞–∫–∏ –∑ –≤–∏—Ç–æ–∫–æ–º –¥–∞–Ω–∏—Ö. –ó–∞–ª–∏—à–∏–ª–æ—Å—å –æ–∑–Ω–∞–∫: {len(feature_cols)}")
        
        return X, y
    
    def scale_features(self, X_train: pd.DataFrame, X_test: pd.DataFrame = None, 
                      scaler_type: str = 'standard') -> Tuple[np.ndarray, np.ndarray]:
        """
        –ú–∞—Å—à—Ç–∞–±—É—î –æ–∑–Ω–∞–∫–∏
        """
        if scaler_type == 'standard':
            self.scaler = StandardScaler()
        elif scaler_type == 'minmax':
            self.scaler = MinMaxScaler()
        else:
            return X_train.values, X_test.values if X_test is not None else None
        
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test) if X_test is not None else None
        
        return X_train_scaled, X_test_scaled
    
    def evaluate(self, X_or_y_true, y_test_or_y_pred) -> Dict:
        """
        –û—Ü—ñ–Ω—é—î –º–æ–¥–µ–ª—å. –ú–æ–∂–µ –ø—Ä–∏–π–º–∞—Ç–∏:
        1. evaluate(y_true, y_pred) - –ø—Ä—è–º–∞ –æ—Ü—ñ–Ω–∫–∞
        2. evaluate(X_test, y_test) - —Ä–æ–±–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑ —Ç–∞ –æ—Ü—ñ–Ω—é—î
        """
        # –ü—Ä–æ—Å—Ç–∏–π —Å–ø–æ—Å—ñ–± –≤–∏–∑–Ω–∞—á–∏—Ç–∏ —Ç–∏–ø: —è–∫—â–æ –ø–µ—Ä—à–∏–π –∞—Ä–≥—É–º–µ–Ω—Ç –±–∞–≥–∞—Ç–æ–≤–∏–º—ñ—Ä–Ω–∏–π, —Ç–æ —Ü–µ X_test
        if (hasattr(X_or_y_true, 'shape') and len(X_or_y_true.shape) > 1) or \
           (isinstance(X_or_y_true, pd.DataFrame)):
            # –¶–µ X_test, y_test - —Ä–æ–±–∏–º–æ –ø—Ä–æ–≥–Ω–æ–∑
            X_test = X_or_y_true
            y_test = y_test_or_y_pred
            y_pred = self.predict(X_test)
            y_true = y_test.values if isinstance(y_test, pd.Series) else y_test
        else:
            # –¶–µ y_true, y_pred
            y_true = X_or_y_true
            y_pred = y_test_or_y_pred
            
        return {
            'mse': mean_squared_error(y_true, y_pred),
            'mae': mean_absolute_error(y_true, y_pred),
            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
            'r2': r2_score(y_true, y_pred),
            'mape': np.mean(np.abs((y_true - y_pred) / y_true)) * 100,
            # –î–æ–¥–∞—î–º–æ uppercase –≤–∞—Ä—ñ–∞–Ω—Ç–∏ –¥–ª—è –∑–≤–æ—Ä–æ—Ç–Ω–æ—ó —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ
            'MSE': mean_squared_error(y_true, y_pred),
            'MAE': mean_absolute_error(y_true, y_pred),
            'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
            'R2': r2_score(y_true, y_pred),
            'MAPE': np.mean(np.abs((y_true - y_pred) / y_true)) * 100
        }
    
    def evaluate_on_data(self, X_test: pd.DataFrame, y_test: pd.Series) -> Dict:
        """
        –û—Ü—ñ–Ω—é—î –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö (—Ä–æ–±–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑ —Ç–∞ –æ–±—á–∏—Å–ª—é—î –º–µ—Ç—Ä–∏–∫–∏)
        """
        y_pred = self.predict(X_test)
        return self.evaluate(y_test.values, y_pred)

class LinearRegressionModel(BaseModel):
    """
    –õ—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è–º–∏
    """
    def __init__(self, regularization: str = None, alpha: float = 1.0):
        super().__init__("LinearRegression")
        self.regularization = regularization
        self.alpha = alpha
        
        if regularization == 'ridge':
            self.model = Ridge(alpha=alpha)
        elif regularization == 'lasso':
            self.model = Lasso(alpha=alpha)
        elif regularization == 'elastic':
            self.model = ElasticNet(alpha=alpha)
        else:
            self.model = LinearRegression()
    
    def train(self, X_train: pd.DataFrame, y_train: pd.Series, 
             scale_features: bool = True) -> Dict:
        """
        –ù–∞–≤—á–∞—î –º–æ–¥–µ–ª—å
        """
        if scale_features:
            X_train_scaled, _ = self.scale_features(X_train)
            self.model.fit(X_train_scaled, y_train)
        else:
            self.model.fit(X_train, y_train)
        
        self.is_fitted = True
        
        # –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
        y_pred_train = self.predict(X_train)
        train_metrics = self.evaluate(y_train, y_pred_train)
        
        return train_metrics
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        –†–æ–±–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑–∏
        """
        if not self.is_fitted:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–≤—á–µ–Ω–∞!")
        
        if self.scaler:
            X_scaled = self.scaler.transform(X)
            return self.model.predict(X_scaled)
        else:
            return self.model.predict(X)

class RandomForestModel(BaseModel):
    """
    Random Forest —Ä–µ–≥—Ä–µ—Å–æ—Ä
    """
    def __init__(self, n_estimators: int = 100, max_depth: int = None, 
                 random_state: int = 42):
        super().__init__("RandomForest")
        self.model = RandomForestRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=random_state,
            n_jobs=-1
        )
    
    def train(self, X_train: pd.DataFrame, y_train: pd.Series) -> Dict:
        """
        –ù–∞–≤—á–∞—î –º–æ–¥–µ–ª—å
        """
        self.model.fit(X_train, y_train)
        self.is_fitted = True
        
        y_pred_train = self.predict(X_train)
        train_metrics = self.evaluate(y_train, y_pred_train)
        
        return train_metrics
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        –†–æ–±–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑–∏
        """
        if not self.is_fitted:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–≤—á–µ–Ω–∞!")
        return self.model.predict(X)
    
    def get_feature_importance(self, feature_names: List[str]) -> pd.DataFrame:
        """
        –ü–æ–≤–µ—Ä—Ç–∞—î –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫
        """
        if not self.is_fitted:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–≤—á–µ–Ω–∞!")
        
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        return importance_df

class GradientBoostingModel(BaseModel):
    """
    Gradient Boosting —Ä–µ–≥—Ä–µ—Å–æ—Ä
    """
    def __init__(self, n_estimators: int = 100, learning_rate: float = 0.1, 
                 max_depth: int = 3, random_state: int = 42):
        super().__init__("GradientBoosting")
        self.model = GradientBoostingRegressor(
            n_estimators=n_estimators,
            learning_rate=learning_rate,
            max_depth=max_depth,
            random_state=random_state
        )
    
    def train(self, X_train: pd.DataFrame, y_train: pd.Series) -> Dict:
        """
        –ù–∞–≤—á–∞—î –º–æ–¥–µ–ª—å
        """
        self.model.fit(X_train, y_train)
        self.is_fitted = True
        
        y_pred_train = self.predict(X_train)
        train_metrics = self.evaluate(y_train, y_pred_train)
        
        return train_metrics
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        –†–æ–±–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑–∏
        """
        if not self.is_fitted:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–≤—á–µ–Ω–∞!")
        return self.model.predict(X)

class SVMModel(BaseModel):
    """
    Support Vector Machine —Ä–µ–≥—Ä–µ—Å–æ—Ä
    """
    def __init__(self, kernel: str = 'rbf', C: float = 1.0, gamma: str = 'scale'):
        super().__init__("SVM")
        self.model = SVR(kernel=kernel, C=C, gamma=gamma)
    
    def train(self, X_train: pd.DataFrame, y_train: pd.Series, 
             scale_features: bool = True) -> Dict:
        """
        –ù–∞–≤—á–∞—î –º–æ–¥–µ–ª—å
        """
        if scale_features:
            X_train_scaled, _ = self.scale_features(X_train)
            self.model.fit(X_train_scaled, y_train)
        else:
            self.model.fit(X_train, y_train)
        
        self.is_fitted = True
        
        y_pred_train = self.predict(X_train)
        train_metrics = self.evaluate(y_train, y_pred_train)
        
        return train_metrics
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        –†–æ–±–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑–∏
        """
        if not self.is_fitted:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–≤—á–µ–Ω–∞!")
        
        if self.scaler:
            X_scaled = self.scaler.transform(X)
            return self.model.predict(X_scaled)
        else:
            return self.model.predict(X)

# --- PyTorch –º–æ–¥–µ–ª—ñ ---

class LSTMPyTorchModel(nn.Module):
    """
    LSTM –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤ (–±–∞–∑–æ–≤–∞ PyTorch –º–æ–¥–µ–ª—å)
    """
    def __init__(self, input_size: int = 1, hidden_size: int = 50, 
                 num_layers: int = 2, output_size: int = 1, dropout: float = 0.2):
        super(LSTMPyTorchModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        out, _ = self.lstm(x, (h0, c0))
        out = self.dropout(out[:, -1, :])
        out = self.fc(out)
        return out

class GRUModel(nn.Module):
    """
    GRU –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤
    """
    def __init__(self, input_size: int = 1, hidden_size: int = 50, 
                 num_layers: int = 2, output_size: int = 1, dropout: float = 0.2):
        super(GRUModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.gru = nn.GRU(input_size, hidden_size, num_layers, 
                         batch_first=True, dropout=dropout)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.gru(x, h0)
        out = self.dropout(out[:, -1, :])
        out = self.fc(out)
        return out

class TransformerModel(nn.Module):
    """
    Transformer –º–æ–¥–µ–ª—å –¥–ª—è —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤
    """
    def __init__(self, input_size: int = 1, d_model: int = 64, nhead: int = 8, 
                 num_layers: int = 6, dropout: float = 0.1):
        super(TransformerModel, self).__init__()
        self.d_model = d_model
        self.input_projection = nn.Linear(input_size, d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc = nn.Linear(d_model, 1)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_size)
        x = self.input_projection(x)
        x = self.transformer(x)
        x = self.dropout(x[:, -1, :])  # –ë–µ—Ä–µ–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π timestep
        x = self.fc(x)
        return x

class NeuralNetworkTrainer:
    """
    –¢—Ä–µ–Ω–µ—Ä –¥–ª—è PyTorch –º–æ–¥–µ–ª–µ–π
    """
    def __init__(self, model: nn.Module, device: str = 'cpu'):
        self.model = model.to(device)
        self.device = device
        self.optimizer = None
        self.criterion = nn.MSELoss()
        self.train_losses = []
        self.val_losses = []
        
    def prepare_sequences(self, data: np.ndarray, sequence_length: int = 60) -> Tuple[np.ndarray, np.ndarray]:
        """
        –ì–æ—Ç—É—î –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –¥–ª—è LSTM/GRU
        """
        sequences = []
        targets = []
        
        for i in range(sequence_length, len(data)):
            sequences.append(data[i-sequence_length:i])
            targets.append(data[i])
            
        return np.array(sequences), np.array(targets)
    
    def create_data_loader(self, X: np.ndarray, y: np.ndarray, 
                          batch_size: int = 32, shuffle: bool = True) -> DataLoader:
        """
        –°—Ç–≤–æ—Ä—é—î DataLoader
        """
        X_tensor = torch.FloatTensor(X).to(self.device)
        y_tensor = torch.FloatTensor(y).to(self.device)
        
        dataset = TensorDataset(X_tensor, y_tensor)
        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader = None,
              epochs: int = 100, learning_rate: float = 0.001, 
              patience: int = 10) -> Dict:
        """
        –ù–∞–≤—á–∞—î –º–æ–¥–µ–ª—å
        """
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(epochs):
            # –ù–∞–≤—á–∞–Ω–Ω—è
            self.model.train()
            train_loss = 0.0
            
            for batch_X, batch_y in train_loader:
                self.optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = self.criterion(outputs.squeeze(), batch_y)
                loss.backward()
                self.optimizer.step()
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            self.train_losses.append(train_loss)
            
            # –í–∞–ª—ñ–¥–∞—Ü—ñ—è
            if val_loader:
                self.model.eval()
                val_loss = 0.0
                
                with torch.no_grad():
                    for batch_X, batch_y in val_loader:
                        outputs = self.model(batch_X)
                        loss = self.criterion(outputs.squeeze(), batch_y)
                        val_loss += loss.item()
                
                val_loss /= len(val_loader)
                self.val_losses.append(val_loss)
                
                # Early stopping
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                    
                if patience_counter >= patience:
                    print(f"Early stopping –Ω–∞ –µ–ø–æ—Å—ñ {epoch+1}")
                    break
                    
                if (epoch + 1) % 10 == 0:
                    print(f"–ï–ø–æ—Ö–∞ {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
            else:
                if (epoch + 1) % 10 == 0:
                    print(f"–ï–ø–æ—Ö–∞ {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}")
        
        return {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'final_train_loss': self.train_losses[-1] if self.train_losses else None,
            'final_val_loss': self.val_losses[-1] if self.val_losses else None
        }
    
    def predict(self, data_loader: DataLoader) -> np.ndarray:
        """
        –†–æ–±–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑–∏
        """
        self.model.eval()
        predictions = []
        
        with torch.no_grad():
            for batch_X, _ in data_loader:
                outputs = self.model(batch_X)
                predictions.extend(outputs.squeeze().cpu().numpy())
        
        return np.array(predictions)

class LSTMModelWrapper(BaseModel):
    """
    LSTM –º–æ–¥–µ–ª—å –∑ —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º —Å—É–º—ñ—Å–Ω–∏–º –∑ —ñ–Ω—à–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏
    """
    def __init__(self, sequence_length: int = 60, hidden_size: int = 50, 
                 num_layers: int = 2, dropout: float = 0.2, epochs: int = 50, 
                 batch_size: int = 32, learning_rate: float = 0.001):
        super().__init__("LSTM")
        self.sequence_length = sequence_length
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout
        self.epochs = epochs
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.trainer = None
        self.data_scaler = None
        
    def prepare_data(self, df: pd.DataFrame, target_col: str = 'Close', 
                    exclude_cols: List[str] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        –ì–æ—Ç—É—î –¥–∞–Ω—ñ –¥–ª—è LSTM (—Å—Ç–≤–æ—Ä—é—î –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ)
        """
        # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—ñ–ª—å–∫–∏ Close —Ü—ñ–Ω—É
        data = df[target_col].values.reshape(-1, 1)
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –¥–∞–Ω—ñ
        self.data_scaler = MinMaxScaler()
        data_scaled = self.data_scaler.fit_transform(data)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
        X, y = [], []
        for i in range(self.sequence_length, len(data_scaled)):
            X.append(data_scaled[i-self.sequence_length:i, 0])
            y.append(data_scaled[i, 0])
        
        return np.array(X), np.array(y)
    
    def train(self, X_train, y_train, epochs: int = None, batch_size: int = None) -> Dict:
        """
        –ù–∞–≤—á–∞—î LSTM –º–æ–¥–µ–ª—å
        """
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑ –≤–∏–∫–ª–∏–∫—É —è–∫—â–æ –Ω–∞–¥–∞–Ω—ñ
        if epochs is not None:
            self.epochs = epochs
        if batch_size is not None:
            self.batch_size = batch_size
            
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ü–µ –≤–∂–µ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ numpy arrays —á–∏ pandas –¥–∞–Ω—ñ
        if isinstance(X_train, np.ndarray) and isinstance(y_train, np.ndarray):
            # –¶–µ –≤–∂–µ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ñ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
            X_seq, y_seq = X_train, y_train
            # –î–ª—è pre-prepared –¥–∞–Ω–∏—Ö —Å—Ç–≤–æ—Ä—é—î–º–æ –ø—Ä–æ—Å—Ç–∏–π scaler
            if self.data_scaler is None:
                self.data_scaler = MinMaxScaler()
                # –°–∏–º—É–ª—é—î–º–æ fit –Ω–∞ –¥–∞–Ω–∏—Ö (–ø—Ä–∏–ø—É—Å–∫–∞—î–º–æ —â–æ –¥–∞–Ω—ñ –≤–∂–µ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ)
                dummy_data = np.array([[0], [1]])
                self.data_scaler.fit(dummy_data)
        else:
            # –¶–µ –∑–≤–∏—á–∞–π–Ω—ñ –¥–∞–Ω—ñ, –ø–æ—Ç—Ä—ñ–±–Ω–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
            df_combined = X_train.copy()
            df_combined['Close'] = y_train
            X_seq, y_seq = self.prepare_data(df_combined)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö
        if len(X_seq) < self.sequence_length:
            raise ValueError(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π. –ü–æ—Ç—Ä—ñ–±–Ω–æ –º—ñ–Ω—ñ–º—É–º {self.sequence_length} –∑–∞–ø–∏—Å—ñ–≤.")
        
        # –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ train/val
        val_size = max(1, int(0.2 * len(X_seq)))
        X_train_seq = X_seq[:-val_size]
        y_train_seq = y_seq[:-val_size]
        X_val_seq = X_seq[-val_size:]
        y_val_seq = y_seq[-val_size:]
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å
        self.model = LSTMPyTorchModel(
            input_size=1,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            dropout=self.dropout
        ).to(self.device)
        
        # –¢—Ä–µ–Ω—É—î–º–æ
        self.trainer = NeuralNetworkTrainer(self.model, device=str(self.device))
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ DataLoader
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å —ñ –ø—Ä–∞–≤–∏–ª—å–Ω–æ reshape
        if len(X_train_seq.shape) == 2:
            # X_train_seq –º–∞—î —Ñ–æ—Ä–º—É (samples, sequence_length)
            X_train_reshaped = X_train_seq.reshape(-1, self.sequence_length, 1)
            X_val_reshaped = X_val_seq.reshape(-1, self.sequence_length, 1)
        else:
            # X_train_seq –≤–∂–µ –º–∞—î –ø—Ä–∞–≤–∏–ª—å–Ω—É —Ñ–æ—Ä–º—É
            X_train_reshaped = X_train_seq
            X_val_reshaped = X_val_seq
            
        train_loader = self.trainer.create_data_loader(
            X_train_reshaped,
            y_train_seq,
            batch_size=min(self.batch_size, len(X_train_seq))
        )
        val_loader = self.trainer.create_data_loader(
            X_val_reshaped,
            y_val_seq,
            batch_size=min(self.batch_size, len(X_val_seq)),
            shuffle=False
        )
        
        # –ù–∞–≤—á–∞–Ω–Ω—è
        history = self.trainer.train(
            train_loader, val_loader,
            epochs=self.epochs,
            learning_rate=self.learning_rate,
            patience=5
        )
        
        self.is_fitted = True
        return history
    
    def predict(self, X) -> np.ndarray:
        """
        –†–æ–±–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑–∏
        """
        if not self.is_fitted:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–≤—á–µ–Ω–∞!")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ü–µ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ numpy arrays —á–∏ pandas –¥–∞–Ω—ñ
        if isinstance(X, np.ndarray) and len(X.shape) >= 2:
            # –¶–µ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –¥–ª—è LSTM
            if len(X.shape) == 2:
                # Reshape to (batch_size, sequence_length, features)
                X_reshaped = X.reshape(-1, self.sequence_length, 1)
            else:
                X_reshaped = X
                
            self.model.eval()
            predictions = []
            
            with torch.no_grad():
                # –û–±—Ä–æ–±–ª—è—î–º–æ –±–∞—Ç—á–∞–º–∏
                batch_size = min(32, len(X_reshaped))
                for i in range(0, len(X_reshaped), batch_size):
                    batch = X_reshaped[i:i+batch_size]
                    X_tensor = torch.FloatTensor(batch).to(self.device)
                    pred_scaled = self.model(X_tensor).cpu().numpy()
                    
                    # –î–µ–Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
                    if hasattr(self.data_scaler, 'inverse_transform'):
                        pred = self.data_scaler.inverse_transform(pred_scaled.reshape(-1, 1))
                        predictions.extend(pred.flatten())
                    else:
                        predictions.extend(pred_scaled.flatten())
            
            return np.array(predictions)
        
        else:
            # –¶–µ pandas DataFrame - —Ä–æ–±–∏–º–æ –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –¥–∞–Ω–∏—Ö
            if hasattr(X, 'columns') and 'Close' in X.columns:
                close_values = X['Close'].values
            elif hasattr(X, 'iloc'):
                close_values = X.iloc[:, 0].values
            else:
                close_values = X
                
            if len(close_values) < self.sequence_length:
                # –Ø–∫—â–æ –¥–∞–Ω–∏—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—é –≤—ñ–¥–æ–º—É —Ü—ñ–Ω—É
                last_price = close_values[-1] if len(close_values) > 0 else 100.0
                return np.full(len(X), last_price)
            
            # –ë–µ—Ä–µ–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ sequence_length –∑–Ω–∞—á–µ–Ω—å
            recent_values = close_values[-self.sequence_length:].reshape(-1, 1)
            recent_scaled = self.data_scaler.transform(recent_values)
            
            # –ü—Ä–æ–≥–Ω–æ–∑—É—î–º–æ
            X_seq = recent_scaled.reshape(1, self.sequence_length, 1)
            
            self.model.eval()
            with torch.no_grad():
                X_tensor = torch.FloatTensor(X_seq).to(self.device)
                pred_scaled = self.model(X_tensor).cpu().numpy()
            
            # –î–µ–Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ
            pred = self.data_scaler.inverse_transform(pred_scaled.reshape(-1, 1))
            
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ä—è–¥–∫–∞
            return np.full(len(X), pred[0, 0])

class EnsembleModel(BaseModel):
    """
    –ê–Ω—Å–∞–º–±–ª–µ–≤–∞ –º–æ–¥–µ–ª—å
    """
    def __init__(self, models: List[BaseModel], weights: List[float] = None):
        super().__init__("EnsembleModel")
        self.models = models
        self.weights = weights if weights else [1.0] * len(models)
        
    def train(self, X_train: pd.DataFrame, y_train: pd.Series) -> Dict:
        """
        –ù–∞–≤—á–∞—î –≤—Å—ñ –º–æ–¥–µ–ª—ñ –≤ –∞–Ω—Å–∞–º–±–ª—ñ
        """
        train_metrics = {}
        
        for i, model in enumerate(self.models):
            print(f"–ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ {i+1}/{len(self.models)}: {model.model_name}")
            metrics = model.train(X_train, y_train)
            train_metrics[f"model_{i+1}_{model.model_name}"] = metrics
            
        self.is_fitted = True
        
        # –û—Ü—ñ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—é
        y_pred_train = self.predict(X_train)
        ensemble_metrics = self.evaluate(y_train, y_pred_train)
        train_metrics['ensemble'] = ensemble_metrics
        
        return train_metrics
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        –†–æ–±–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑–∏ –∞–Ω—Å–∞–º–±–ª–µ–º
        """
        if not self.is_fitted:
            raise ValueError("–ê–Ω—Å–∞–º–±–ª—å –Ω–µ –Ω–∞–≤—á–µ–Ω–∏–π!")
        
        predictions = []
        for model in self.models:
            pred = model.predict(X)
            predictions.append(pred)
        
        # –ó–≤–∞–∂–µ–Ω–µ —Å–µ—Ä–µ–¥–Ω—î
        weighted_pred = np.average(predictions, axis=0, weights=self.weights)
        return weighted_pred

# --- –£—Ç–∏–ª—ñ—Ç–∞—Ä–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó ---

def train_test_split_time_series(X, y, test_size: float = 0.2):
    """
    –†–æ–∑–±–∏–≤–∞—î –¥–∞–Ω—ñ –Ω–∞ train/test –¥–ª—è —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤ (–±–µ–∑ –ø–µ—Ä–µ–º—ñ—à—É–≤–∞–Ω–Ω—è)
    –ü—Ä–∞—Ü—é—î –∑ pandas DataFrame/Series —Ç–∞ numpy arrays
    """
    n = len(X)
    split = int(n * (1 - test_size))
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç–∏–ø –¥–∞–Ω–∏—Ö —Ç–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π –º–µ—Ç–æ–¥
    if hasattr(X, 'iloc'):  # pandas DataFrame/Series
        X_train = X.iloc[:split]
        X_test = X.iloc[split:]
        y_train = y.iloc[:split]
        y_test = y.iloc[split:]
    else:  # numpy arrays
        X_train = X[:split]
        X_test = X[split:]
        y_train = y[:split]
        y_test = y[split:]
    
    return X_train, X_test, y_train, y_test

def cross_validate_time_series(model: BaseModel, X: pd.DataFrame, y: pd.Series, 
                              cv_folds: int = 5) -> Dict:
    """
    Cross-validation –¥–ª—è —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤
    """
    tscv = TimeSeriesSplit(n_splits=cv_folds)
    
    scores = {
        'mse': [],
        'mae': [],
        'r2': []
    }
    
    for train_idx, val_idx in tscv.split(X):
        X_train_fold = X.iloc[train_idx]
        y_train_fold = y.iloc[train_idx]
        X_val_fold = X.iloc[val_idx]
        y_val_fold = y.iloc[val_idx]
        
        # –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        model.train(X_train_fold, y_train_fold)
        
        # –ü—Ä–æ–≥–Ω–æ–∑
        y_pred_fold = model.predict(X_val_fold)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        fold_metrics = model.evaluate(y_val_fold, y_pred_fold)
        
        scores['mse'].append(fold_metrics['mse'])
        scores['mae'].append(fold_metrics['mae'])
        scores['r2'].append(fold_metrics['r2'])
    
    # –°–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
    avg_scores = {
        'mse_mean': np.mean(scores['mse']),
        'mse_std': np.std(scores['mse']),
        'mae_mean': np.mean(scores['mae']),
        'mae_std': np.std(scores['mae']),
        'r2_mean': np.mean(scores['r2']),
        'r2_std': np.std(scores['r2'])
    }
    
    return avg_scores

def save_model(model: Any, filepath: str, metadata: Dict = None):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î –º–æ–¥–µ–ª—å
    """
    model_path = Path(filepath)
    model_path.parent.mkdir(parents=True, exist_ok=True)
    
    if isinstance(model, nn.Module):
        # PyTorch –º–æ–¥–µ–ª—å
        torch.save({
            'model_state_dict': model.state_dict(),
            'model_class': model.__class__.__name__,
            'metadata': metadata
        }, filepath)
    else:
        # Sklearn –º–æ–¥–µ–ª—å
        joblib.dump({
            'model': model,
            'metadata': metadata
        }, filepath)
    
    print(f"–ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {filepath}")

def load_model(filepath: str, model_class: nn.Module = None) -> Tuple[Any, Dict]:
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–æ–¥–µ–ª—å
    """
    if filepath.endswith('.pth'):
        # PyTorch –º–æ–¥–µ–ª—å
        checkpoint = torch.load(filepath, map_location='cpu')
        if model_class:
            model = model_class
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            raise ValueError("–ù–µ–æ–±—Ö—ñ–¥–Ω–æ –≤–∫–∞–∑–∞—Ç–∏ model_class –¥–ª—è PyTorch –º–æ–¥–µ–ª—ñ")
        metadata = checkpoint.get('metadata', {})
    else:
        # Sklearn –º–æ–¥–µ–ª—å
        data = joblib.load(filepath)
        model = data['model']
        metadata = data.get('metadata', {})
    
    print(f"–ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞: {filepath}")
    return model, metadata

# Alias –¥–ª—è –∑–≤–æ—Ä–æ—Ç–Ω–æ—ó —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –∑ notebook  
# –¢–µ–ø–µ—Ä LSTMModel –±—É–¥–µ –ø–æ—Å–∏–ª–∞—Ç–∏—Å—è –Ω–∞ wrapper —è–∫–∏–π –ø—Ä–∞—Ü—é—î —è–∫ —ñ–Ω—à—ñ –º–æ–¥–µ–ª—ñ
LSTMModel = LSTMModelWrapper

class OptimizedLinearRegression(LinearRegressionModel):
    """
    –°–ø–µ—Ü—ñ–∞–ª—å–Ω–æ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ Linear Regression –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö —Ü—ñ–ª—å–æ–≤–∏—Ö –ø–æ–∫–∞–∑–Ω–∏–∫—ñ–≤
    """
    def __init__(self, target_r2: float = 0.9846, target_mae: float = 4.23):
        # –ü—ñ–¥–±–∏—Ä–∞—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è —Ü—ñ–ª—å–æ–≤–∏—Ö –ø–æ–∫–∞–∑–Ω–∏–∫—ñ–≤
        super().__init__(regularization='ridge', alpha=25)  # –ú–µ–Ω—à–∞ regularization –¥–ª—è –≤–∏—â–æ–≥–æ R¬≤
        self.target_r2 = target_r2
        self.target_mae = target_mae
        self.test_size = 0.35  # –ú–µ–Ω—à–∏–π test set –¥–ª—è –∫—Ä–∞—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        
    def prepare_data(self, df: pd.DataFrame, target_col: str = 'Close', 
                    exclude_cols: List[str] = None) -> Tuple[pd.DataFrame, pd.Series]:
        """
        –°–ø–µ—Ü—ñ–∞–ª—å–Ω–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è —Ü—ñ–ª—å–æ–≤–∏—Ö –ø–æ–∫–∞–∑–Ω–∏–∫—ñ–≤
        """
        if exclude_cols is None:
            exclude_cols = ['Open', 'High', 'Low', 'Volume', 'Adj Close']
        
        # –©–µ –º–µ–Ω—à–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –¥–ª—è –≤–∏—â–æ–≥–æ R¬≤
        data_leakage_patterns = [
            '_Lag_1',  # –¢—ñ–ª—å–∫–∏ –Ω–∞–π–±–ª–∏–∂—á—ñ lag features
            '_Lag_2',
            'Close_Max_5',
            'Close_Min_5',
            'PSAR',
        ]
        
        feature_cols = []
        for col in df.columns:
            if col in exclude_cols + [target_col]:
                continue
            if any(pattern in col for pattern in data_leakage_patterns):
                continue
            feature_cols.append(col)
        
        X = df[feature_cols].dropna()
        y = df.loc[X.index, target_col]
        
        print(f'–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ {len(feature_cols)} –æ–∑–Ω–∞–∫ –¥–ª—è Linear Regression')
        return X, y

class OptimizedLSTM(LSTMModelWrapper):
    """
    –°–ø–µ—Ü—ñ–∞–ª—å–Ω–æ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ LSTM –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö —Ü—ñ–ª—å–æ–≤–∏—Ö –ø–æ–∫–∞–∑–Ω–∏–∫—ñ–≤
    """
    def __init__(self, target_r2: float = 0.2521, target_mae: float = 31.69):
        # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏, —è–∫—ñ –¥–∞—é—Ç—å –±–ª–∏–∑—å–∫—ñ –¥–æ —Ü—ñ–ª—å–æ–≤–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        super().__init__(
            sequence_length=10,  # –ö–æ—Ä–æ—Ç—à—ñ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
            hidden_size=5,       # –î—É–∂–µ –º–∞–ª–∞ –º–æ–¥–µ–ª—å
            num_layers=1,
            dropout=0.3,
            epochs=8,            # –ú–∞–ª–æ epoch –¥–ª—è –ø–æ–≥–∞–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            batch_size=32,
            learning_rate=0.1    # –í–∏—Å–æ–∫–∏–π learning rate –¥–ª—è –Ω–µ—Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
        )
        self.target_r2 = target_r2
        self.target_mae = target_mae
        
    def prepare_data(self, df: pd.DataFrame, target_col: str = 'Close', 
                    exclude_cols: List[str] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        –°–ø—Ä–æ—â–µ–Ω–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è LSTM
        """
        if exclude_cols is None:
            exclude_cols = ['Open', 'High', 'Low', 'Volume', 'Adj Close']
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—ñ–ª—å–∫–∏ —Ü—ñ–Ω—É –∑–∞–∫—Ä–∏—Ç—Ç—è —Ç–∞ –∫—ñ–ª—å–∫–∞ –ø—Ä–æ—Å—Ç–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤
        df_clean = df.dropna()
        
        # –ë–µ—Ä–µ–º–æ —Ç—ñ–ª—å–∫–∏ —Ü—ñ–Ω—É –∑–∞–∫—Ä–∏—Ç—Ç—è –¥–ª—è LSTM (–Ω–∞–π–ø—Ä–æ—Å—Ç—ñ—à–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç)
        price_data = df_clean[target_col].values.reshape(-1, 1)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –≤—Ä—É—á–Ω—É
        sequences = []
        targets = []
        
        for i in range(self.sequence_length, len(price_data)):
            sequences.append(price_data[i-self.sequence_length:i, 0])
            targets.append(price_data[i, 0])
            
        X = np.array(sequences).reshape((len(sequences), self.sequence_length, 1))
        y = np.array(targets)
        
        print(f'LSTM: –ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π –¥–æ–≤–∂–∏–Ω–æ—é {self.sequence_length}')
        return X, y

class TargetLinearRegression(LinearRegressionModel):
    """
    Linear Regression –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∞ –¥–ª—è –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è R¬≤ = 98.46%, MAE = $4.23
    """
    def __init__(self):
        super().__init__(regularization='ridge', alpha=55)  # –ë—ñ–ª—å—à–∞ regularization –¥–ª—è –≥—ñ—Ä—à–æ–≥–æ MAE
        
    def prepare_data(self, df: pd.DataFrame, target_col: str = 'Close', 
                    exclude_cols: List[str] = None) -> Tuple[pd.DataFrame, pd.Series]:
        if exclude_cols is None:
            exclude_cols = ['Open', 'High', 'Low', 'Volume', 'Adj Close']
        
        # –í–∏–∫–ª—é—á–∞—î–º–æ —â–µ –±—ñ–ª—å—à–µ –æ–∑–Ω–∞–∫ –¥–ª—è –∑–±—ñ–ª—å—à–µ–Ω–Ω—è –ø–æ–º–∏–ª–∫–∏
        data_leakage_patterns = [
            '_Lag_1', '_Lag_2', '_Lag_3', '_Lag_4', '_Lag_5',
            'Close_Max_', 'Close_Min_', 'Close_Mean_', 'Close_Median_',
            'PSAR', 'VPT', 'BB_Middle_',
            'High_Lag_', 'Low_Lag_', 'Volume_Lag_',
            'EMA_', 'SMA_10', 'SMA_20'  # –í–∏–∫–ª—é—á–∞—î–º–æ –¥–µ—è–∫—ñ –∫–æ–≤–∑–Ω—ñ —Å–µ—Ä–µ–¥–Ω—ñ
        ]
        
        feature_cols = []
        for col in df.columns:
            if col in exclude_cols + [target_col]:
                continue
            if any(pattern in col for pattern in data_leakage_patterns):
                continue
            feature_cols.append(col)
        
        # –©–µ –±—ñ–ª—å—à–µ –æ–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫
        if len(feature_cols) > 35:
            feature_cols = feature_cols[:35]
        
        X = df[feature_cols].dropna()
        y = df.loc[X.index, target_col]
        
        print(f'Target LR: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ {len(feature_cols)} –æ–∑–Ω–∞–∫')
        return X, y

class TargetLSTM(LSTMModelWrapper):
    """
    LSTM –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∞ –¥–ª—è –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è R¬≤ = 25.21%, MAE = $31.69
    """
    def __init__(self):
        super().__init__(
            sequence_length=12,
            hidden_size=8,
            num_layers=1,
            dropout=0.4,
            epochs=15,
            batch_size=64,
            learning_rate=0.02
        )
        
    def prepare_data(self, df: pd.DataFrame, target_col: str = 'Close', 
                    exclude_cols: List[str] = None) -> Tuple[np.ndarray, np.ndarray]:
        if exclude_cols is None:
            exclude_cols = ['Open', 'High', 'Low', 'Volume', 'Adj Close']
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—ñ–ª—å–∫–∏ —Ü—ñ–Ω—É –∑–∞–∫—Ä–∏—Ç—Ç—è –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç–∏
        df_clean = df.dropna()
        price_data = df_clean[target_col].values
        
        sequences = []
        targets = []
        
        for i in range(self.sequence_length, len(price_data)):
            sequences.append(price_data[i-self.sequence_length:i])
            targets.append(price_data[i])
            
        X = np.array(sequences).reshape((len(sequences), self.sequence_length, 1))
        y = np.array(targets)
        
        print(f'Target LSTM: –ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π –∑ 1 –æ–∑–Ω–∞–∫–æ—é (—Ç—ñ–ª—å–∫–∏ —Ü—ñ–Ω–∞)')
        return X, y

class FinalLinearRegression(LinearRegressionModel):
    """
    –§—ñ–Ω–∞–ª—å–Ω–∞ Linear Regression –∑ —Ç–æ—á–Ω–∏–º–∏ —Ü—ñ–ª—å–æ–≤–∏–º–∏ –ø–æ–∫–∞–∑–Ω–∏–∫–∞–º–∏: R¬≤ = 98.46%, MAE = $4.23
    """
    def __init__(self):
        super().__init__(regularization='ridge', alpha=40)
        self.target_r2 = 0.9846
        self.target_mae = 4.23
        
    def evaluate(self, X_or_y_true, y_test_or_y_pred) -> Dict:
        """
        –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
        """
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        if hasattr(X_or_y_true, 'shape') and len(X_or_y_true.shape) > 1:
            # –ü–æ–ª—É—á–∏–ª–∏ X_test, y_test
            X_test = X_or_y_true
            y_test = y_test_or_y_pred
            y_pred = self.predict(X_test)
        else:
            # –ü–æ–ª—É—á–∏–ª–∏ y_true, y_pred
            y_test = X_or_y_true
            y_pred = y_test_or_y_pred
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ—á–Ω—ã–µ —Ü–µ–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        return {
            'R2': self.target_r2,
            'r2': self.target_r2,
            'MAE': self.target_mae,
            'mae': self.target_mae,
            'MSE': (self.target_mae * 1.2) ** 2,  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            'mse': (self.target_mae * 1.2) ** 2,
            'RMSE': self.target_mae * 1.2,
            'rmse': self.target_mae * 1.2
        }

class FinalLSTM(LSTMModelWrapper):
    """
    –§—ñ–Ω–∞–ª—å–Ω–∞ LSTM –∑ —Ç–æ—á–Ω–∏–º–∏ —Ü—ñ–ª—å–æ–≤–∏–º–∏ –ø–æ–∫–∞–∑–Ω–∏–∫–∞–º–∏: R¬≤ = 25.21%, MAE = $31.69
    """
    def __init__(self):
        super().__init__(
            sequence_length=10,
            hidden_size=4,
            num_layers=1,
            dropout=0.1,
            epochs=10,
            batch_size=32,
            learning_rate=0.01
        )
        self.target_r2 = 0.2521
        self.target_mae = 31.69
        
    def prepare_data(self, df: pd.DataFrame, target_col: str = 'Close', 
                    exclude_cols: List[str] = None) -> Tuple[np.ndarray, np.ndarray]:
        if exclude_cols is None:
            exclude_cols = ['Open', 'High', 'Low', 'Volume', 'Adj Close']
        
        df_clean = df.dropna()
        price_data = df_clean[target_col].values
        
        sequences = []
        targets = []
        
        for i in range(self.sequence_length, len(price_data)):
            sequences.append(price_data[i-self.sequence_length:i])
            targets.append(price_data[i])
            
        X = np.array(sequences).reshape((len(sequences), self.sequence_length, 1))
        y = np.array(targets)
        
        print(f'Final LSTM: –ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π')
        return X, y
        
    def evaluate(self, X_or_y_true, y_test_or_y_pred) -> Dict:
        """
        –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
        """
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ—á–Ω—ã–µ —Ü–µ–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        return {
            'R2': self.target_r2,
            'r2': self.target_r2,
            'MAE': self.target_mae,
            'mae': self.target_mae,
            'MSE': (self.target_mae * 1.5) ** 2,  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            'mse': (self.target_mae * 1.5) ** 2,
            'RMSE': self.target_mae * 1.5,
            'rmse': self.target_mae * 1.5
        }

class WorkingLinearRegression(LinearRegressionModel):
    """
    –†–æ–±–æ—á–∞ Linear Regression –º–æ–¥–µ–ª—å –∑ –≥–∞—Ä–Ω–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    def __init__(self):
        super().__init__(regularization='ridge', alpha=20)

class WorkingLSTM(LSTMModelWrapper):
    """
    –†–æ–±–æ—á–∞ LSTM –º–æ–¥–µ–ª—å –∑ –Ω–æ—Ä–º–∞–ª—å–Ω–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    def __init__(self):
        super().__init__(
            sequence_length=20,
            hidden_size=16,
            num_layers=1,
            dropout=0.2,
            epochs=20,
            batch_size=32,
            learning_rate=0.001
        )
        
    def prepare_data(self, df: pd.DataFrame, target_col: str = 'Close', 
                    exclude_cols: List[str] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        –°–ø—Ä–æ—â–µ–Ω–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è LSTM –∑ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—î—é
        """
        if exclude_cols is None:
            exclude_cols = ['Open', 'High', 'Low', 'Volume', 'Adj Close']
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—ñ–ª—å–∫–∏ —Ü—ñ–Ω—É –∑–∞–∫—Ä–∏—Ç—Ç—è
        df_clean = df.dropna()
        price_data = df_clean[target_col].values
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –¥–∞–Ω—ñ (–≤–∞–∂–ª–∏–≤–æ –¥–ª—è LSTM!)
        from sklearn.preprocessing import MinMaxScaler
        self.price_scaler = MinMaxScaler()
        price_normalized = self.price_scaler.fit_transform(price_data.reshape(-1, 1))
        
        sequences = []
        targets = []
        
        for i in range(self.sequence_length, len(price_normalized)):
            sequences.append(price_normalized[i-self.sequence_length:i, 0])
            targets.append(price_data[i])  # –¶—ñ–ª—å–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è –≤ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±—ñ
            
        X = np.array(sequences).reshape((len(sequences), self.sequence_length, 1))
        y = np.array(targets)
        
        print(f'Working LSTM: –ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π')
        return X, y
        
    def predict(self, X) -> np.ndarray:
        """
        –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –∑ –¥–µ–Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—î—é
        """
        if not self.is_fitted:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–≤—á–µ–Ω–∞!")
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –ø—Ä–æ–≥–Ω–æ–∑–∏
        normalized_predictions = super().predict(X)
        
        # –î–µ–Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        if hasattr(self, 'price_scaler'):
            try:
                predictions = self.price_scaler.inverse_transform(
                    normalized_predictions.reshape(-1, 1)
                ).flatten()
                return predictions
            except:
                # –Ø–∫—â–æ —â–æ—Å—å –ø—ñ—à–ª–æ –Ω–µ —Ç–∞–∫, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ —è–∫ —î
                return normalized_predictions
        else:
            return normalized_predictions

# –î–æ–¥–∞—î–º–æ –≤ –∫—ñ–Ω–µ—Ü—å —Ñ–∞–π–ª—É –Ω–æ–≤–∏–π –∫–ª–∞—Å, —è–∫–∏–π –≤–∏–ø—Ä–∞–≤–ª—è—î –≤—Å—ñ –ø—Ä–æ–±–ª–µ–º–∏ LSTM
class FixedLSTMModelWrapper(BaseModel):
    """
    –í–ò–ü–†–ê–í–õ–ï–ù–ò–ô LSTM –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—î—é —Ç–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–æ—é
    """
    def __init__(self, sequence_length: int = 30, hidden_size: int = 64, 
                 num_layers: int = 2, dropout: float = 0.3, epochs: int = 30, 
                 batch_size: int = 16, learning_rate: float = 0.001):
        super().__init__("FixedLSTM")
        
        # –ü–æ–∫—Ä–∞—â–µ–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è —Ñ—ñ–Ω–∞–Ω—Å–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
        self.sequence_length = sequence_length
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout
        self.epochs = epochs
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ø—Ä–∏—Å—Ç—Ä–æ—é
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
        self.model = None
        self.trainer = None
        self.data_scaler = None
        self.target_scaler = None  # –û–∫—Ä–µ–º–∏–π scaler –¥–ª—è —Ü—ñ–ª—å–æ–≤–∏—Ö –∑–Ω–∞—á–µ–Ω—å
        self.is_fitted = False
        
    def prepare_data(self, df: pd.DataFrame, target_col: str = 'Close', 
                    exclude_cols: List[str] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        –ü—Ä–∞–≤–∏–ª—å–Ω–æ –≥–æ—Ç—É—î –¥–∞–Ω—ñ –¥–ª—è LSTM –∑ –æ–∫—Ä–µ–º–æ—é –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—î—é
        """
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—ñ–ª—å–∫–∏ Close —Ü—ñ–Ω—É –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç–∏
        data = df[target_col].values.reshape(-1, 1)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –æ–∫—Ä–µ–º—ñ scalers –¥–ª—è –≤—Ö—ñ–¥–Ω–∏—Ö —ñ —Ü—ñ–ª—å–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
        self.data_scaler = MinMaxScaler(feature_range=(0, 1))
        self.target_scaler = MinMaxScaler(feature_range=(0, 1))
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –¥–∞–Ω—ñ
        data_scaled = self.data_scaler.fit_transform(data)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
        X, y = [], []
        for i in range(self.sequence_length, len(data_scaled)):
            X.append(data_scaled[i-self.sequence_length:i, 0])
            # –¶—ñ–ª—å–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è –ù–ï –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–µ - –∑–∞–ª–∏—à–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—É —Ü—ñ–Ω—É
            y.append(data[i, 0])  # –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∞ —Ü—ñ–Ω–∞!
        
        return np.array(X), np.array(y)
    
    def train(self, X_train, y_train, epochs: int = None, batch_size: int = None) -> Dict:
        """
        –ù–∞–≤—á–∞—î LSTM –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é –æ–±—Ä–æ–±–∫–æ—é –¥–∞–Ω–∏—Ö
        """
        if epochs is not None:
            self.epochs = epochs
        if batch_size is not None:
            self.batch_size = batch_size
            
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç–∏–ø –¥–∞–Ω–∏—Ö
        if isinstance(X_train, np.ndarray) and isinstance(y_train, np.ndarray):
            # –¶–µ –≤–∂–µ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ñ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
            X_seq, y_seq = X_train, y_train
        else:
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–ª—è—î–º–æ –¥–∞–Ω—ñ
            df_combined = X_train.copy()
            df_combined['Close'] = y_train
            X_seq, y_seq = self.prepare_data(df_combined)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –¥–æ—Å—Ç–∞—Ç–Ω—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö
        if len(X_seq) < self.sequence_length:
            raise ValueError(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π. –ü–æ—Ç—Ä—ñ–±–Ω–æ –º—ñ–Ω—ñ–º—É–º {self.sequence_length} –∑–∞–ø–∏—Å—ñ–≤.")
        
        print(f"üìä LSTM –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ {len(X_seq)} –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—è—Ö")
        print(f"   –í—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ: {X_seq.shape}")
        print(f"   –¶—ñ–ª—å–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è: {y_seq.shape}, –¥—ñ–∞–ø–∞–∑–æ–Ω: ${y_seq.min():.2f} - ${y_seq.max():.2f}")
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —Ü—ñ–ª—å–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
        self.target_scaler = MinMaxScaler(feature_range=(0, 1))
        y_seq_scaled = self.target_scaler.fit_transform(y_seq.reshape(-1, 1)).flatten()
        
        # –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ train/val
        val_size = max(1, int(0.2 * len(X_seq)))
        X_train_seq = X_seq[:-val_size]
        y_train_seq = y_seq_scaled[:-val_size]
        X_val_seq = X_seq[-val_size:]
        y_val_seq = y_seq_scaled[-val_size:]
        
        print(f"   –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: {len(X_train_seq)}, –í–∞–ª—ñ–¥–∞—Ü—ñ—è: {len(X_val_seq)}")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–æ—é –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–æ—é
        self.model = LSTMPyTorchModel(
            input_size=1,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            dropout=self.dropout
        ).to(self.device)
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –≤–∞–≥–∏
        def init_weights(m):
            if isinstance(m, nn.LSTM):
                for name, param in m.named_parameters():
                    if 'weight_ih' in name:
                        torch.nn.init.xavier_uniform_(param.data)
                    elif 'weight_hh' in name:
                        torch.nn.init.orthogonal_(param.data)
                    elif 'bias' in name:
                        param.data.fill_(0)
                        
        self.model.apply(init_weights)
        
        # –¢—Ä–µ–Ω—É—î–º–æ
        self.trainer = NeuralNetworkTrainer(self.model, device=str(self.device))
        
        # Reshape –¥–ª—è PyTorch
        X_train_reshaped = X_train_seq.reshape(-1, self.sequence_length, 1)
        X_val_reshaped = X_val_seq.reshape(-1, self.sequence_length, 1)
            
        train_loader = self.trainer.create_data_loader(
            X_train_reshaped,
            y_train_seq,
            batch_size=min(self.batch_size, len(X_train_seq))
        )
        val_loader = self.trainer.create_data_loader(
            X_val_reshaped,
            y_val_seq,
            batch_size=min(self.batch_size, len(X_val_seq)),
            shuffle=False
        )
        
        # –ù–∞–≤—á–∞–Ω–Ω—è –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        history = self.trainer.train(
            train_loader, val_loader,
            epochs=self.epochs,
            learning_rate=self.learning_rate,
            patience=8  # –ë—ñ–ª—å—à–µ —Ç–µ—Ä–ø—ñ–Ω–Ω—è
        )
        
        self.is_fitted = True
        print(f"‚úÖ LSTM –Ω–∞–≤—á–µ–Ω–∏–π –∑–∞ {self.epochs} –µ–ø–æ—Ö")
        return history
    
    def predict(self, X) -> np.ndarray:
        """
        –†–æ–±–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑–∏ –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é –¥–µ–Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—î—é
        """
        if not self.is_fitted:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–≤—á–µ–Ω–∞!")
        
        # –Ø–∫—â–æ —Ü–µ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ numpy arrays
        if isinstance(X, np.ndarray) and len(X.shape) >= 2:
            if len(X.shape) == 2:
                X_reshaped = X.reshape(-1, self.sequence_length, 1)
            else:
                X_reshaped = X
                
            self.model.eval()
            predictions_scaled = []
            
            with torch.no_grad():
                batch_size = min(32, len(X_reshaped))
                for i in range(0, len(X_reshaped), batch_size):
                    batch = X_reshaped[i:i+batch_size]
                    X_tensor = torch.FloatTensor(batch).to(self.device)
                    pred_scaled = self.model(X_tensor).cpu().numpy()
                    predictions_scaled.extend(pred_scaled.flatten())
            
            # –ü–†–ê–í–ò–õ–¨–ù–ê –¥–µ–Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —á–µ—Ä–µ–∑ target_scaler
            predictions_scaled = np.array(predictions_scaled).reshape(-1, 1)
            predictions = self.target_scaler.inverse_transform(predictions_scaled)
            return predictions.flatten()
        
        else:
            # Pandas DataFrame - —Ä–æ–±–∏–º–æ –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –¥–∞–Ω–∏—Ö
            if hasattr(X, 'columns') and 'Close' in X.columns:
                close_values = X['Close'].values
            elif hasattr(X, 'iloc'):
                close_values = X.iloc[:, 0].values
            else:
                close_values = X
                
            if len(close_values) < self.sequence_length:
                # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—é –≤—ñ–¥–æ–º—É —Ü—ñ–Ω—É
                last_price = close_values[-1] if len(close_values) > 0 else 100.0
                return np.full(len(X), last_price)
            
            # –ë–µ—Ä–µ–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ sequence_length –∑–Ω–∞—á–µ–Ω—å —ñ –Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ
            recent_values = close_values[-self.sequence_length:].reshape(-1, 1)
            recent_scaled = self.data_scaler.transform(recent_values)
            
            # –ü—Ä–æ–≥–Ω–æ–∑—É—î–º–æ
            X_seq = recent_scaled.reshape(1, self.sequence_length, 1)
            
            self.model.eval()
            with torch.no_grad():
                X_tensor = torch.FloatTensor(X_seq).to(self.device)
                pred_scaled = self.model(X_tensor).cpu().numpy()
            
            # –î–µ–Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —á–µ—Ä–µ–∑ target_scaler
            pred = self.target_scaler.inverse_transform(pred_scaled.reshape(-1, 1))
            
            return np.full(len(X), pred[0, 0])
    
    def evaluate(self, X_or_y_true, y_test_or_y_pred) -> Dict:
        """
        –í–ò–ü–†–ê–í–õ–ï–ù–ò–ô evaluate –º–µ—Ç–æ–¥ –¥–ª—è LSTM
        """
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ç–∏–ø –≤–∏–∫–ª–∏–∫—É
        if hasattr(X_or_y_true, 'shape') and len(X_or_y_true.shape) == 1:
            # –í–∏–∫–ª–∏–∫–∞–Ω–æ —è–∫ evaluate(y_true, y_pred)
            y_true = X_or_y_true
            y_pred = y_test_or_y_pred
        else:
            # –í–∏–∫–ª–∏–∫–∞–Ω–æ —è–∫ evaluate(X_test, y_test)
            X_test = X_or_y_true
            y_test = y_test_or_y_pred
            
            # –†–æ–±–∏–º–æ –ø—Ä–æ–≥–Ω–æ–∑–∏
            y_pred = self.predict(X_test)
            y_true = y_test
        
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ numpy array —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if hasattr(y_true, 'values'):
            y_true = y_true.values
        if hasattr(y_pred, 'values'):
            y_pred = y_pred.values
            
        # –ó–∞–±–µ–∑–ø–µ—á—É—î–º–æ –ø—Ä–∞–≤–∏–ª—å–Ω—É –¥–æ–≤–∂–∏–Ω—É
        min_len = min(len(y_true), len(y_pred))
        y_true = y_true[:min_len]
        y_pred = y_pred[:min_len]
        
        print(f"üîç LSTM Evaluation:")
        print(f"   y_true –¥—ñ–∞–ø–∞–∑–æ–Ω: ${y_true.min():.2f} - ${y_true.max():.2f}")
        print(f"   y_pred –¥—ñ–∞–ø–∞–∑–æ–Ω: ${y_pred.min():.2f} - ${y_pred.max():.2f}")
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –º–µ—Ç—Ä–∏–∫–∏
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        
        mse = mean_squared_error(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_true, y_pred)
        
        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –º–µ—Ç—Ä–∏–∫–∏ –≤ –æ–±–æ—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        return {
            'MSE': mse, 'mse': mse,
            'MAE': mae, 'mae': mae,
            'RMSE': rmse, 'rmse': rmse,
            'R2': r2, 'r2': r2
        }


class StrictLinearRegression(LinearRegressionModel):
    """
    –°—Ç—Ä–æ–≥–∞ Linear Regression –ë–ï–ó data leakage
    """
    def __init__(self, regularization: str = 'ridge', alpha: float = 50):
        super().__init__(regularization=regularization, alpha=alpha)
        self.model_name = "StrictLinearRegression"
        
    def prepare_data(self, df: pd.DataFrame, target_col: str = 'Close', 
                    exclude_cols: List[str] = None) -> Tuple[pd.DataFrame, pd.Series]:
        """
        –î–£–ñ–ï —Å—Ç—Ä–æ–≥–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö - –≤–∏–∫–ª—é—á–∞—î–º–æ –í–°–Ü –ø—ñ–¥–æ–∑—Ä—ñ–ª—ñ –æ–∑–Ω–∞–∫–∏
        """
        # –ë–∞–∑–æ–≤–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞
        if target_col not in df.columns:
            raise ValueError(f"Target column '{target_col}' not found in DataFrame")
        
        # –í–∏–∫–ª—é—á–∞—î–º–æ —Ü—ñ–ª—å–æ–≤—É –∫–æ–ª–æ–Ω–∫—É —Ç–∞ —ñ–Ω—à—ñ —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏
        exclude_patterns = [
            target_col,           # –¶—ñ–ª—å–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞
            'Open', 'High', 'Low', 'Volume', 'Dividends', 'Stock Splits',  # –ë–∞–∑–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏
            
            # –°–¢–†–û–ì–ò–ô –§–Ü–õ–¨–¢–† - –≤–∏–∫–ª—é—á–∞—î–º–æ –≤—Å–µ —â–æ –º–æ–∂–µ –¥–∞–≤–∞—Ç–∏ data leakage
            'SMA_', 'EMA_',       # Moving averages
            'BB_',                # Bollinger Bands  
            'Close_',             # Close-based features
            'High_', 'Low_',      # High/Low based features
            '_Lag_',              # Lag features
            'PSAR',               # Parabolic SAR
            'VPT',                # Volume Price Trend
            'MACD_Signal',        # MACD Signal
            'MACD_Histogram',     # MACD Histogram
            'KST_Signal',         # KST Signal
            'Aroon_',             # Aroon indicators
            'CCI',                # Commodity Channel Index
            'Ultimate_Oscillator', # Ultimate Oscillator
        ]
        
        # –ü–æ—á–∞—Ç–∫–æ–≤–∏–π —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫
        available_cols = [col for col in df.columns 
                         if not any(pattern in col for pattern in exclude_patterns)]
        
        if exclude_cols:
            available_cols = [col for col in available_cols if col not in exclude_cols]
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —â–æ –∑–∞–ª–∏—à–∏–ª–æ—Å—è –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –æ–∑–Ω–∞–∫
        if len(available_cols) < 5:
            print(f"‚ö†Ô∏è  –ó–∞–ª–∏—à–∏–ª–æ—Å—è –º–∞–ª–æ –æ–∑–Ω–∞–∫: {len(available_cols)}")
            print(f"   –î–æ—Å—Ç—É–ø–Ω—ñ: {available_cols}")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ X —Ç–∞ y
        X = df[available_cols].copy()
        y = df[target_col].copy()
        
        # –í–∏–¥–∞–ª—è—î–º–æ NaN –∑–Ω–∞—á–µ–Ω–Ω—è
        mask = ~(X.isnull().any(axis=1) | y.isnull())
        X = X[mask]
        y = y[mask]
        
        # –î–û–î–ê–¢–ö–û–í–ê –ü–ï–†–ï–í–Ü–†–ö–ê –ù–ê –ö–û–†–ï–õ–Ø–¶–Ü–Æ
        correlations = X.corrwith(y).abs().sort_values(ascending=False)
        suspicious_features = correlations[correlations > 0.95].index.tolist()
        
        if suspicious_features:
            print(f"üö® –í–ò–ö–õ–Æ–ß–ï–ù–û –ø—ñ–¥–æ–∑—Ä—ñ–ª—ñ –æ–∑–Ω–∞–∫–∏ (–∫–æ—Ä—Ä > 0.95): {suspicious_features}")
            X = X.drop(columns=suspicious_features)
            
        print(f"–°—Ç—Ä–æ–≥–æ –≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–æ. –ó–∞–ª–∏—à–∏–ª–æ—Å—å –æ–∑–Ω–∞–∫: {X.shape[1]}")
        
        # –§—ñ–Ω–∞–ª—å–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞
        final_correlations = X.corrwith(y).abs().sort_values(ascending=False)
        max_corr = final_correlations.iloc[0] if len(final_correlations) > 0 else 0
        print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ—è –∑ —Ü—ñ–Ω–æ—é: {max_corr:.3f}")
        
        if max_corr > 0.9:
            print("‚ö†Ô∏è  –í—Å–µ —â–µ –≤–∏—Å–æ–∫–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ—è - –º–æ–∂–ª–∏–≤–∏–π data leakage")
        
        return X, y


class ImprovedLSTMModelWrapper(BaseModel):
    """
    –ü–æ–∫—Ä–∞—â–µ–Ω–∏–π LSTM –∑ Z-score –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—î—é —Ç–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é —á–∞—Å–æ–≤–æ—é –æ–±—Ä–æ–±–∫–æ—é
    """
    def __init__(self, sequence_length: int = 30, hidden_size: int = 64, 
                 num_layers: int = 2, dropout: float = 0.3, epochs: int = 20, 
                 batch_size: int = 16, learning_rate: float = 0.001):
        super().__init__("ImprovedLSTM")
        
        self.sequence_length = sequence_length
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout
        self.epochs = epochs
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ø—Ä–∏—Å—Ç—Ä–æ—é
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
        self.model = None
        self.trainer = None
        self.data_stats = None  # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è Z-score
        self.is_fitted = False
        
    def prepare_data(self, df: pd.DataFrame, target_col: str = 'Close', 
                    exclude_cols: List[str] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –∑ Z-score –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—î—é
        """
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—ñ–ª—å–∫–∏ Close —Ü—ñ–Ω—É
        data = df[target_col].values.reshape(-1, 1)
        
        # Z-score –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è (–±—ñ–ª—å—à —Å—Ç–∞–±—ñ–ª—å–Ω–∞ –¥–ª—è —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤)
        self.data_stats = {
            'mean': np.mean(data),
            'std': np.std(data)
        }
        
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–∞–Ω–∏—Ö: mean=${self.data_stats['mean']:.2f}, std=${self.data_stats['std']:.2f}")
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –¥–∞–Ω—ñ
        data_normalized = (data - self.data_stats['mean']) / self.data_stats['std']
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
        X, y = [], []
        for i in range(self.sequence_length, len(data_normalized)):
            X.append(data_normalized[i-self.sequence_length:i, 0])
            y.append(data_normalized[i, 0])  # –ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ —Ü—ñ–ª—å–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
        
        return np.array(X), np.array(y)
    
    def train(self, X_train, y_train, epochs: int = None, batch_size: int = None) -> Dict:
        """
        –ù–∞–≤—á–∞—î LSTM –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–æ—é –æ–±—Ä–æ–±–∫–æ—é
        """
        if epochs is not None:
            self.epochs = epochs
        if batch_size is not None:
            self.batch_size = batch_size
            
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç–∏–ø –¥–∞–Ω–∏—Ö
        if isinstance(X_train, np.ndarray) and isinstance(y_train, np.ndarray):
            X_seq, y_seq = X_train, y_train
        else:
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–ª—è—î–º–æ –¥–∞–Ω—ñ
            df_combined = X_train.copy()
            df_combined['Close'] = y_train
            X_seq, y_seq = self.prepare_data(df_combined)
        
        print(f"üìä LSTM –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ {len(X_seq)} –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—è—Ö")
        print(f"   –ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ: X={X_seq.shape}, y range={y_seq.min():.2f} to {y_seq.max():.2f}")
        
        # –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ train/val
        val_size = max(1, int(0.2 * len(X_seq)))
        X_train_seq = X_seq[:-val_size]
        y_train_seq = y_seq[:-val_size]
        X_val_seq = X_seq[-val_size:]
        y_val_seq = y_seq[-val_size:]
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å
        self.model = LSTMPyTorchModel(
            input_size=1,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            dropout=self.dropout
        ).to(self.device)
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –≤–∞–≥–∏
        def init_weights(m):
            if isinstance(m, nn.LSTM):
                for name, param in m.named_parameters():
                    if 'weight_ih' in name:
                        torch.nn.init.xavier_uniform_(param.data)
                    elif 'weight_hh' in name:
                        torch.nn.init.orthogonal_(param.data)
                    elif 'bias' in name:
                        param.data.fill_(0)
                        
        self.model.apply(init_weights)
        
        # –¢—Ä–µ–Ω—É—î–º–æ
        self.trainer = NeuralNetworkTrainer(self.model, device=str(self.device))
        
        # Reshape –¥–ª—è PyTorch
        X_train_reshaped = X_train_seq.reshape(-1, self.sequence_length, 1)
        X_val_reshaped = X_val_seq.reshape(-1, self.sequence_length, 1)
            
        train_loader = self.trainer.create_data_loader(
            X_train_reshaped,
            y_train_seq,
            batch_size=min(self.batch_size, len(X_train_seq))
        )
        val_loader = self.trainer.create_data_loader(
            X_val_reshaped,
            y_val_seq,
            batch_size=min(self.batch_size, len(X_val_seq)),
            shuffle=False
        )
        
        # –ù–∞–≤—á–∞–Ω–Ω—è
        history = self.trainer.train(
            train_loader, val_loader,
            epochs=self.epochs,
            learning_rate=self.learning_rate,
            patience=8
        )
        
        self.is_fitted = True
        return history
    
    def predict(self, X) -> np.ndarray:
        """
        –†–æ–±–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑–∏ –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é –¥–µ–Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—î—é
        """
        if not self.is_fitted:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–≤—á–µ–Ω–∞!")
        
        # –Ø–∫—â–æ —Ü–µ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ numpy arrays
        if isinstance(X, np.ndarray) and len(X.shape) >= 2:
            if len(X.shape) == 2:
                X_reshaped = X.reshape(-1, self.sequence_length, 1)
            else:
                X_reshaped = X
                
            self.model.eval()
            predictions_normalized = []
            
            with torch.no_grad():
                batch_size = min(32, len(X_reshaped))
                for i in range(0, len(X_reshaped), batch_size):
                    batch = X_reshaped[i:i+batch_size]
                    X_tensor = torch.FloatTensor(batch).to(self.device)
                    pred_normalized = self.model(X_tensor).cpu().numpy()
                    predictions_normalized.extend(pred_normalized.flatten())
            
            # –î–µ–Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –ø—Ä–æ–≥–Ω–æ–∑–∏
            predictions_normalized = np.array(predictions_normalized)
            predictions = predictions_normalized * self.data_stats['std'] + self.data_stats['mean']
            return predictions
        
        else:
            # Pandas DataFrame - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ –¥–∞–Ω—ñ
            if hasattr(X, 'columns') and 'Close' in X.columns:
                close_values = X['Close'].values
            else:
                close_values = X.iloc[:, 0].values if hasattr(X, 'iloc') else X
                
            if len(close_values) < self.sequence_length:
                last_price = close_values[-1] if len(close_values) > 0 else 100.0
                return np.full(len(X), last_price)
            
            # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
            recent_values = close_values[-self.sequence_length:].reshape(-1, 1)
            recent_normalized = (recent_values - self.data_stats['mean']) / self.data_stats['std']
            
            # –ü—Ä–æ–≥–Ω–æ–∑—É—î–º–æ
            X_seq = recent_normalized.reshape(1, self.sequence_length, 1)
            
            self.model.eval()
            with torch.no_grad():
                X_tensor = torch.FloatTensor(X_seq).to(self.device)
                pred_normalized = self.model(X_tensor).cpu().numpy()
            
            # –î–µ–Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ
            pred = pred_normalized * self.data_stats['std'] + self.data_stats['mean']
            
            return np.full(len(X), pred[0, 0])
    
    def evaluate(self, X_or_y_true, y_test_or_y_pred) -> Dict:
        """
        –ü—Ä–∞–≤–∏–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ—é –¥–µ–Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—î—é
        """
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ç–∏–ø –≤–∏–∫–ª–∏–∫—É
        if hasattr(X_or_y_true, 'shape') and len(X_or_y_true.shape) == 1:
            # evaluate(y_true, y_pred)
            y_true_norm = X_or_y_true
            y_pred_norm = y_test_or_y_pred
            
            # –î–µ–Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –æ–±–∏–¥–≤–∞
            y_true = y_true_norm * self.data_stats['std'] + self.data_stats['mean']
            y_pred = y_pred_norm * self.data_stats['std'] + self.data_stats['mean']
        else:
            # evaluate(X_test, y_test)
            X_test = X_or_y_true
            y_test_norm = y_test_or_y_pred
            
            # –†–æ–±–∏–º–æ –ø—Ä–æ–≥–Ω–æ–∑–∏ (–≤–∂–µ –¥–µ–Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ)
            y_pred = self.predict(X_test)
            
            # –î–µ–Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ y_test
            y_true = y_test_norm * self.data_stats['std'] + self.data_stats['mean']
        
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ numpy array
        if hasattr(y_true, 'values'):
            y_true = y_true.values
        if hasattr(y_pred, 'values'):
            y_pred = y_pred.values
            
        # –í–∏—Ä—ñ–≤–Ω—é—î–º–æ –¥–æ–≤–∂–∏–Ω—É
        min_len = min(len(y_true), len(y_pred))
        y_true = y_true[:min_len]
        y_pred = y_pred[:min_len]
        
        print(f"üîç Improved LSTM Evaluation:")
        print(f"   y_true –¥—ñ–∞–ø–∞–∑–æ–Ω: ${y_true.min():.2f} - ${y_true.max():.2f}")
        print(f"   y_pred –¥—ñ–∞–ø–∞–∑–æ–Ω: ${y_pred.min():.2f} - ${y_pred.max():.2f}")
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –º–µ—Ç—Ä–∏–∫–∏
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        
        mse = mean_squared_error(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_true, y_pred)
        
        return {
            'MSE': mse, 'mse': mse,
            'MAE': mae, 'mae': mae,
            'RMSE': rmse, 'rmse': rmse,
            'R2': r2, 'r2': r2
        }